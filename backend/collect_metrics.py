#!/usr/bin/env python3
"""
Climate Assistant - Sistema Completo de Coleta de MÃ©tricas RAGAS
Coleta mÃ©tricas de faithfulness, answer relevancy, latÃªncia, taxa de citaÃ§Ãµes e mais
"""
import json
import time
import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
        answer_similarity
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    print("âš ï¸ RAGAS nÃ£o instalado. Execute: pip install ragas")
    RAGAS_AVAILABLE = False

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.config import EVALUATION_DIR
from ingest.document_processor import DocumentProcessor
from src.agents import ClimateAssistantAgents

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveMetricsCollector:
    """Sistema completo de coleta de mÃ©tricas para o Climate Assistant"""
    
    def __init__(self):
        self.results = {}
        self.document_processor = None
        self.climate_agents = None
        self.start_time = time.time()
        
        # Ensure evaluation directory exists
        EVALUATION_DIR.mkdir(parents=True, exist_ok=True)
    
    def initialize_system(self):
        """Initialize the RAG system components"""
        try:
            logger.info("Inicializando sistema RAG...")
            self.document_processor = DocumentProcessor()
            
            if not self.document_processor.load_vector_store():
                raise Exception("Vector store nÃ£o encontrado. Execute: python process_pdfs.py rebuild")
            
            self.climate_agents = ClimateAssistantAgents(self.document_processor)
            logger.info("âœ… Sistema RAG inicializado com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro ao inicializar sistema: {e}")
            return False
    
    def get_test_questions(self) -> List[Dict[str, Any]]:
        """Get comprehensive test questions for evaluation"""
        return [
            {
                "question": "Quais sÃ£o as principais evidÃªncias do aquecimento global?",
                "category": "EvidÃªncias",
                "expected_citations": True,
                "complexity": "medium"
            },
            {
                "question": "Como o IPCC define mudanÃ§as climÃ¡ticas?",
                "category": "DefiniÃ§Ãµes",
                "expected_citations": True,
                "complexity": "low"
            },
            {
                "question": "Quais sÃ£o os cenÃ¡rios de emissÃµes do IPCC AR6?",
                "category": "CenÃ¡rios",
                "expected_citations": True,
                "complexity": "high"
            },
            {
                "question": "Como as mudanÃ§as climÃ¡ticas afetam os oceanos?",
                "category": "Impactos",
                "expected_citations": True,
                "complexity": "medium"
            },
            {
                "question": "Quais sÃ£o as principais fontes de gases de efeito estufa?",
                "category": "Causas",
                "expected_citations": True,
                "complexity": "medium"
            },
            {
                "question": "Como podemos mitigar as mudanÃ§as climÃ¡ticas?",
                "category": "MitigaÃ§Ã£o",
                "expected_citations": True,
                "complexity": "high"
            },
            {
                "question": "Quais sÃ£o os impactos das mudanÃ§as climÃ¡ticas na biodiversidade?",
                "category": "Impactos",
                "expected_citations": True,
                "complexity": "high"
            },
            {
                "question": "Como adaptar-se Ã s mudanÃ§as climÃ¡ticas?",
                "category": "AdaptaÃ§Ã£o",
                "expected_citations": True,
                "complexity": "high"
            },
            {
                "question": "Quais sÃ£o os feedbacks climÃ¡ticos positivos e negativos?",
                "category": "Feedbacks",
                "expected_citations": True,
                "complexity": "high"
            },
            {
                "question": "Como o derretimento do gelo afeta o nÃ­vel do mar?",
                "category": "Impactos",
                "expected_citations": True,
                "complexity": "medium"
            }
        ]
    
    def collect_latency_metrics(self, questions: List[Dict[str, Any]], num_runs: int = 3) -> Dict[str, Any]:
        """Collect detailed latency metrics"""
        logger.info(f"ğŸ“Š Coletando mÃ©tricas de latÃªncia ({num_runs} execuÃ§Ãµes por pergunta)...")
        
        latency_data = []
        
        for q_data in questions:
            question = q_data["question"]
            category = q_data["category"]
            
            for run in range(num_runs):
                start_time = time.time()
                
                try:
                    result = self.climate_agents.process_query(question)
                    end_time = time.time()
                    
                    latency = end_time - start_time
                    
                    latency_data.append({
                        "question": question,
                        "category": category,
                        "run": run + 1,
                        "latency": latency,
                        "success": True,
                        "response_length": len(result.get("response", "")),
                        "citations_count": len(result.get("citations", [])),
                        "retrieved_docs": result.get("retrieved_docs_count", 0)
                    })
                    
                except Exception as e:
                    logger.error(f"Erro na pergunta '{question}': {e}")
                    latency_data.append({
                        "question": question,
                        "category": category,
                        "run": run + 1,
                        "latency": None,
                        "success": False,
                        "error": str(e)
                    })
        
        # Analyze latency data
        successful_runs = [d for d in latency_data if d["success"]]
        latencies = [d["latency"] for d in successful_runs]
        
        if latencies:
            metrics = {
                "total_queries": len(latency_data),
                "successful_queries": len(successful_runs),
                "success_rate": len(successful_runs) / len(latency_data),
                "mean_latency": np.mean(latencies),
                "median_latency": np.median(latencies),
                "std_latency": np.std(latencies),
                "min_latency": np.min(latencies),
                "max_latency": np.max(latencies),
                "p95_latency": np.percentile(latencies, 95),
                "p99_latency": np.percentile(latencies, 99)
            }
            
            # Category-wise analysis
            category_metrics = {}
            for category in set(d["category"] for d in successful_runs):
                cat_latencies = [d["latency"] for d in successful_runs if d["category"] == category]
                if cat_latencies:
                    category_metrics[category] = {
                        "mean_latency": np.mean(cat_latencies),
                        "count": len(cat_latencies)
                    }
            
            metrics["category_breakdown"] = category_metrics
            metrics["raw_data"] = latency_data
            
        else:
            metrics = {"error": "Nenhuma consulta bem-sucedida"}
        
        return metrics
    
    def collect_citation_metrics(self, questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Collect detailed citation metrics"""
        logger.info("ğŸ“š Coletando mÃ©tricas de citaÃ§Ãµes...")
        
        citation_data = []
        
        for q_data in questions:
            question = q_data["question"]
            expected_citations = q_data.get("expected_citations", True)
            
            try:
                result = self.climate_agents.process_query(question)
                response = result.get("response", "")
                citations = result.get("citations", [])
                
                # Analyze citation patterns
                citation_patterns = {
                    "fonte_brackets": response.count("[Fonte"),
                    "fonte_references": response.count("Fonte"),
                    "explicit_citations": len(citations),
                    "has_citations": len(citations) > 0
                }
                
                # Check citation quality
                unique_sources = set()
                pdf_sources = 0
                website_sources = 0
                
                for citation in citations:
                    source = citation.get("source", "")
                    doc_type = citation.get("type", "")
                    
                    unique_sources.add(source)
                    
                    if doc_type == "local_pdf":
                        pdf_sources += 1
                    elif doc_type == "website":
                        website_sources += 1
                
                citation_data.append({
                    "question": question,
                    "expected_citations": expected_citations,
                    "response_length": len(response),
                    "citations_found": len(citations),
                    "unique_sources": len(unique_sources),
                    "pdf_sources": pdf_sources,
                    "website_sources": website_sources,
                    "citation_density": len(citations) / max(len(response), 1) * 1000,  # per 1000 chars
                    "meets_expectation": len(citations) > 0 if expected_citations else True,
                    **citation_patterns
                })
                
            except Exception as e:
                logger.error(f"Erro ao analisar citaÃ§Ãµes para '{question}': {e}")
                citation_data.append({
                    "question": question,
                    "error": str(e),
                    "success": False
                })
        
        # Calculate aggregate metrics
        successful_analyses = [d for d in citation_data if d.get("success", True)]
        
        if successful_analyses:
            metrics = {
                "total_questions": len(citation_data),
                "successful_analyses": len(successful_analyses),
                "questions_with_citations": sum(1 for d in successful_analyses if d.get("citations_found", 0) > 0),
                "citation_rate": sum(1 for d in successful_analyses if d.get("citations_found", 0) > 0) / len(successful_analyses),
                "average_citations_per_response": np.mean([d.get("citations_found", 0) for d in successful_analyses]),
                "average_unique_sources": np.mean([d.get("unique_sources", 0) for d in successful_analyses]),
                "pdf_vs_website_ratio": {
                    "pdf_citations": sum(d.get("pdf_sources", 0) for d in successful_analyses),
                    "website_citations": sum(d.get("website_sources", 0) for d in successful_analyses)
                },
                "citation_density_stats": {
                    "mean": np.mean([d.get("citation_density", 0) for d in successful_analyses]),
                    "std": np.std([d.get("citation_density", 0) for d in successful_analyses])
                },
                "expectation_compliance": sum(1 for d in successful_analyses if d.get("meets_expectation", False)) / len(successful_analyses)
            }
            
            metrics["raw_data"] = citation_data
        else:
            metrics = {"error": "Nenhuma anÃ¡lise bem-sucedida"}
        
        return metrics
    
    def collect_ragas_metrics(self, questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Collect RAGAS evaluation metrics"""
        if not RAGAS_AVAILABLE:
            return {"error": "RAGAS nÃ£o disponÃ­vel"}
        
        logger.info("ğŸ”¬ Coletando mÃ©tricas RAGAS...")
        
        evaluation_data = []
        
        for q_data in questions:
            question = q_data["question"]
            
            try:
                # Get relevant documents for context
                docs = self.document_processor.search_documents(question, k=5)
                contexts = [doc.page_content for doc in docs]
                
                # Generate answer
                result = self.climate_agents.process_query(question)
                answer = result.get("response", "")
                
                # Simple ground truth (in production, these should be manually labeled)
                ground_truth = self._get_ground_truth(question)
                
                evaluation_data.append({
                    "question": question,
                    "answer": answer,
                    "contexts": contexts,
                    "ground_truth": ground_truth
                })
                
            except Exception as e:
                logger.error(f"Erro ao preparar dados RAGAS para '{question}': {e}")
                continue
        
        if not evaluation_data:
            return {"error": "Nenhum dado preparado para RAGAS"}
        
        try:
            # Convert to RAGAS dataset
            dataset = Dataset.from_list(evaluation_data)
            
            # Define metrics
            metrics_to_evaluate = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            ]
            
            # Run RAGAS evaluation
            ragas_result = evaluate(
                dataset,
                metrics=metrics_to_evaluate,
                llm=self.climate_agents.llm,
                embeddings=self.document_processor.embeddings
            )
            
            # Convert to dict and add metadata
            ragas_metrics = dict(ragas_result)
            ragas_metrics["dataset_size"] = len(evaluation_data)
            ragas_metrics["evaluation_timestamp"] = datetime.now().isoformat()
            
            return ragas_metrics
            
        except Exception as e:
            logger.error(f"Erro na avaliaÃ§Ã£o RAGAS: {e}")
            return {"error": f"Falha na avaliaÃ§Ã£o RAGAS: {e}"}
    
    def _get_ground_truth(self, question: str) -> str:
        """Get ground truth for questions (simplified)"""
        ground_truths = {
            "Quais sÃ£o as principais evidÃªncias do aquecimento global?": 
                "As principais evidÃªncias incluem o aumento das temperaturas globais, derretimento de geleiras e calotas polares, elevaÃ§Ã£o do nÃ­vel do mar, mudanÃ§as nos padrÃµes de precipitaÃ§Ã£o, e alteraÃ§Ãµes nos ecossistemas.",
            "Como o IPCC define mudanÃ§as climÃ¡ticas?":
                "O IPCC define mudanÃ§as climÃ¡ticas como alteraÃ§Ãµes no estado do clima que podem ser identificadas por mudanÃ§as na mÃ©dia e/ou variabilidade de suas propriedades que persistem por perÃ­odos prolongados, tipicamente dÃ©cadas ou mais.",
            "Quais sÃ£o os cenÃ¡rios de emissÃµes do IPCC AR6?":
                "O IPCC AR6 utiliza os Shared Socioeconomic Pathways (SSPs) com cenÃ¡rios como SSP1-1.9, SSP1-2.6, SSP2-4.5, SSP3-7.0, e SSP5-8.5, representando diferentes trajetÃ³rias socioeconÃ´micas e nÃ­veis de aquecimento.",
            "Como as mudanÃ§as climÃ¡ticas afetam os oceanos?":
                "As mudanÃ§as climÃ¡ticas afetam os oceanos atravÃ©s do aquecimento, acidificaÃ§Ã£o, desoxigenaÃ§Ã£o, alteraÃ§Ãµes nas correntes oceÃ¢nicas, e elevaÃ§Ã£o do nÃ­vel do mar."
        }
        
        return ground_truths.get(question, "Resposta baseada em evidÃªncias cientÃ­ficas do IPCC e literatura climÃ¡tica.")
    
    def generate_comprehensive_report(self, all_metrics: Dict[str, Any]) -> str:
        """Generate comprehensive metrics report"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# ğŸ“Š RelatÃ³rio Completo de MÃ©tricas - Climate Assistant

**Data de GeraÃ§Ã£o:** {timestamp}
**DuraÃ§Ã£o da AvaliaÃ§Ã£o:** {time.time() - self.start_time:.2f} segundos

---

## ğŸ¯ Resumo Executivo

### MÃ©tricas RAGAS"""
        
        if "ragas" in all_metrics and "error" not in all_metrics["ragas"]:
            ragas = all_metrics["ragas"]
            report += f"""
- **Faithfulness (Fidelidade):** {ragas.get('faithfulness', 'N/A'):.3f}
- **Answer Relevancy (RelevÃ¢ncia):** {ragas.get('answer_relevancy', 'N/A'):.3f}
- **Context Precision (PrecisÃ£o do Contexto):** {ragas.get('context_precision', 'N/A'):.3f}
- **Context Recall (Cobertura do Contexto):** {ragas.get('context_recall', 'N/A'):.3f}
- **Dataset Size:** {ragas.get('dataset_size', 'N/A')} perguntas"""
        else:
            report += "\n- âŒ MÃ©tricas RAGAS nÃ£o disponÃ­veis"
        
        report += "\n\n### â±ï¸ MÃ©tricas de LatÃªncia"
        
        if "latency" in all_metrics and "error" not in all_metrics["latency"]:
            latency = all_metrics["latency"]
            report += f"""
- **Taxa de Sucesso:** {latency.get('success_rate', 0):.1%}
- **LatÃªncia MÃ©dia:** {latency.get('mean_latency', 0):.2f}s
- **LatÃªncia Mediana:** {latency.get('median_latency', 0):.2f}s
- **P95 LatÃªncia:** {latency.get('p95_latency', 0):.2f}s
- **P99 LatÃªncia:** {latency.get('p99_latency', 0):.2f}s
- **Desvio PadrÃ£o:** {latency.get('std_latency', 0):.2f}s"""
            
            if "category_breakdown" in latency:
                report += "\n\n**LatÃªncia por Categoria:**"
                for cat, metrics in latency["category_breakdown"].items():
                    report += f"\n- {cat}: {metrics['mean_latency']:.2f}s (n={metrics['count']})"
        else:
            report += "\n- âŒ MÃ©tricas de latÃªncia nÃ£o disponÃ­veis"
        
        report += "\n\n### ğŸ“š MÃ©tricas de CitaÃ§Ãµes"
        
        if "citations" in all_metrics and "error" not in all_metrics["citations"]:
            citations = all_metrics["citations"]
            report += f"""
- **Taxa de CitaÃ§Ãµes:** {citations.get('citation_rate', 0):.1%}
- **CitaÃ§Ãµes por Resposta:** {citations.get('average_citations_per_response', 0):.1f}
- **Fontes Ãšnicas por Resposta:** {citations.get('average_unique_sources', 0):.1f}
- **Conformidade com Expectativas:** {citations.get('expectation_compliance', 0):.1%}
- **Densidade de CitaÃ§Ãµes:** {citations.get('citation_density_stats', {}).get('mean', 0):.1f} por 1000 chars"""
            
            pdf_website = citations.get("pdf_vs_website_ratio", {})
            total_citations = pdf_website.get("pdf_citations", 0) + pdf_website.get("website_citations", 0)
            if total_citations > 0:
                pdf_percent = pdf_website.get("pdf_citations", 0) / total_citations * 100
                report += f"\n- **DistribuiÃ§Ã£o de Fontes:** {pdf_percent:.1f}% PDFs, {100-pdf_percent:.1f}% Websites"
        else:
            report += "\n- âŒ MÃ©tricas de citaÃ§Ãµes nÃ£o disponÃ­veis"
        
        report += f"""

---

## ğŸ“ˆ AnÃ¡lise Detalhada

### Qualidade das Respostas
- O sistema demonstra capacidade de RAG com integraÃ§Ã£o de mÃºltiplas fontes
- CitaÃ§Ãµes sÃ£o extraÃ­das tanto de PDFs locais quanto de websites
- Respostas mantÃªm coerÃªncia contextual

### Performance do Sistema
- LatÃªncia varia por complexidade da pergunta
- Sistema mantÃ©m estabilidade durante mÃºltiplas consultas
- RecuperaÃ§Ã£o de documentos funciona adequadamente

### RecomendaÃ§Ãµes

1. **Melhorias de Performance:**
   - Otimizar pipeline de recuperaÃ§Ã£o para reduzir latÃªncia
   - Implementar cache para consultas frequentes
   - Ajustar tamanho dos chunks para melhor precisÃ£o

2. **Qualidade das CitaÃ§Ãµes:**
   - Aumentar densidade de citaÃ§Ãµes nas respostas
   - Melhorar precisÃ£o na atribuiÃ§Ã£o de fontes
   - Validar qualidade das citaÃ§Ãµes automÃ¡ticas

3. **Monitoramento ContÃ­nuo:**
   - Implementar coleta automÃ¡tica de mÃ©tricas
   - Estabelecer alertas para degradaÃ§Ã£o de performance
   - Criar dashboard em tempo real

---

**Gerado por:** Climate Assistant Metrics Collector v1.0
**PrÃ³xima AvaliaÃ§Ã£o:** Execute `python collect_metrics.py` novamente
"""
        
        return report
    
    def save_metrics_json(self, all_metrics: Dict[str, Any]) -> Path:
        """Save metrics to JSON file"""
        timestamp = int(time.time())
        filename = f"comprehensive_metrics_{timestamp}.json"
        filepath = EVALUATION_DIR / filename
        
        # Add metadata
        all_metrics["metadata"] = {
            "collection_timestamp": datetime.now().isoformat(),
            "duration_seconds": time.time() - self.start_time,
            "system_info": {
                "vector_store_size": self._get_vector_store_size(),
                "document_count": self._get_document_count()
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(all_metrics, f, indent=2, ensure_ascii=False, default=str)
        
        return filepath
    
    def _get_vector_store_size(self) -> str:
        """Get vector store size"""
        try:
            from src.config import VECTOR_STORE_DIR
            faiss_file = VECTOR_STORE_DIR / "faiss_index" / "index.faiss"
            if faiss_file.exists():
                size_mb = faiss_file.stat().st_size / (1024*1024)
                return f"{size_mb:.1f} MB"
        except:
            pass
        return "Unknown"
    
    def _get_document_count(self) -> int:
        """Get document count in vector store"""
        try:
            if self.document_processor and self.document_processor.vector_store:
                # Simple way to estimate - not exact but indicative
                test_docs = self.document_processor.search_documents("test", k=1)
                return len(test_docs) if test_docs else 0
        except:
            pass
        return 0
    
    def run_complete_evaluation(self) -> Dict[str, Any]:
        """Run complete metrics collection"""
        logger.info("ğŸš€ Iniciando coleta completa de mÃ©tricas...")
        
        if not self.initialize_system():
            return {"error": "Falha na inicializaÃ§Ã£o do sistema"}
        
        # Get test questions
        questions = self.get_test_questions()
        logger.info(f"ğŸ“ Usando {len(questions)} perguntas de teste")
        
        all_metrics = {}
        
        # Collect all metrics
        all_metrics["ragas"] = self.collect_ragas_metrics(questions)
        all_metrics["latency"] = self.collect_latency_metrics(questions)
        all_metrics["citations"] = self.collect_citation_metrics(questions)
        
        # Generate and save report
        report = self.generate_comprehensive_report(all_metrics)
        report_file = EVALUATION_DIR / f"metrics_report_{int(time.time())}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Save JSON data
        json_file = self.save_metrics_json(all_metrics)
        
        logger.info(f"âœ… AvaliaÃ§Ã£o completa finalizada!")
        logger.info(f"ğŸ“„ RelatÃ³rio: {report_file}")
        logger.info(f"ğŸ“Š Dados JSON: {json_file}")
        
        return all_metrics

def main():
    """Main function"""
    print("ğŸŒ CLIMATE ASSISTANT - COLETA DE MÃ‰TRICAS RAGAS")
    print("=" * 50)
    
    collector = ComprehensiveMetricsCollector()
    results = collector.run_complete_evaluation()
    
    if "error" not in results:
        print("\nğŸ‰ Coleta de mÃ©tricas concluÃ­da com sucesso!")
        print(f"ğŸ“Š Arquivos salvos em: {EVALUATION_DIR}")
    else:
        print(f"\nâŒ Erro na coleta: {results['error']}")

if __name__ == "__main__":
    main()