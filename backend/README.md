# Climate Assistant RAG System

Um assistente inteligente com RAG (Retrieval-Augmented Generation) + Agentes para quest√µes de meio ambiente e mudan√ßas clim√°ticas, baseado em documentos do IPCC.

## üåç Vis√£o Geral

Este sistema implementa uma prova de conceito (PoC) de um assistente com:
- **RAG**: Recupera√ß√£o de informa√ß√µes de documentos cient√≠ficos
- **Agentes LangGraph**: Orquestra√ß√£o inteligente de m√∫ltiplos agentes especializados
- **Anti-alucina√ß√£o**: Sistema de verifica√ß√£o e cita√ß√µes obrigat√≥rias
- **Foco Ambiental**: Especializado em mudan√ßas clim√°ticas e meio ambiente

## üèóÔ∏è Arquitetura do Sistema

### Arquitetura do Grafo de Agentes

O sistema utiliza LangGraph para orquestrar um pipeline de agentes especializados:

```mermaid
graph TD
    A[Supervisor] --> B[Retriever]
    B --> C[Answerer]
    C --> D[Self-check]
    D --> E{Qualidade OK?}
    E -->|Sim| F[Safety]
    E -->|N√£o| G{Tentativas < 3?}
    G -->|Sim| B
    G -->|N√£o| H[End]
    F --> I[Finalizer]
    I --> H
```

#### Agentes Especializados

1. **Supervisor**: Roteamento e gerenciamento de consultas
2. **Retriever**: Busca e recupera√ß√£o de documentos relevantes
3. **Answerer**: Gera√ß√£o de respostas com cita√ß√µes obrigat√≥rias
4. **Self-check**: Verifica√ß√£o de qualidade e evid√™ncias
5. **Safety**: Adi√ß√£o de disclaimers e verifica√ß√µes de seguran√ßa
6. **Finalizer**: Formata√ß√£o final

### Stack Tecnol√≥gica
- **Python 3.8+**
- **LangChain + LangGraph**: Orquestra√ß√£o de agentes
- **Ollama**: LLM open-weights (Llama 3.1 8B, Qwen2.5 7B, etc.)
- **FAISS/Chroma**: Armazenamento vetorial
- **HuggingFace Embeddings**: Embeddings de texto
- **Flask**: API REST compat√≠vel com frontend existente
- **RAGAS**: Sistema de avalia√ß√£o de qualidade

## üìÅ Estrutura do Projeto

```
backend/
‚îú‚îÄ‚îÄ README.md              
‚îú‚îÄ‚îÄ LICENSE                
‚îú‚îÄ‚îÄ CITATION.cff         
‚îú‚îÄ‚îÄ Dockerfile            
‚îú‚îÄ‚îÄ requirements.txt     
‚îú‚îÄ‚îÄ setup.py             
‚îú‚îÄ‚îÄ src/                 
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agents.py      
‚îÇ   ‚îî‚îÄ‚îÄ config.py        
‚îú‚îÄ‚îÄ app/                 
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ app.py          
‚îú‚îÄ‚îÄ ingest/              
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ document_processor.py
‚îú‚îÄ‚îÄ eval/                
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ tests/               
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_agents.py
‚îÇ   ‚îú‚îÄ‚îÄ test_document_processor.py
‚îÇ   ‚îî‚îÄ‚îÄ test_config.py
‚îú‚îÄ‚îÄ data/                
‚îÇ   ‚îú‚îÄ‚îÄ documents/      
‚îÇ   ‚îú‚îÄ‚îÄ vector_store/   
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/     
‚îî‚îÄ‚îÄ static/             
    ‚îú‚îÄ‚îÄ pln.html
    ‚îú‚îÄ‚îÄ seta.png
    ‚îî‚îÄ‚îÄ styles.css
```

## üöÄ Setup e Instala√ß√£o

### Pr√©-requisitos
- Python 3.8 ou superior
- Ollama instalado e configurado
- 8GB+ RAM recomendado

### 1. Instala√ß√£o Autom√°tica
```bash
cd backend
python setup.py
```

### 2. Instala√ß√£o Manual

#### Instalar depend√™ncias Python:
```bash
pip install -r requirements.txt
```

#### Instalar e configurar Ollama:
```bash
# Instalar Ollama (https://ollama.ai/)
# Baixar modelo LLM
ollama pull llama3.1:8b
# ou
ollama pull qwen2.5:7b
```

#### Processar documentos:
```bash
python -m ingest.document_processor
```

#### Iniciar servidor:
```bash
python -m app.app
```

### 3. Executar Testes

```bash
# Executar todos os testes
python -m pytest tests/

# Executar testes espec√≠ficos
python -m pytest tests/test_agents.py
python -m pytest tests/test_document_processor.py
python -m pytest tests/test_config.py
```

## üìä Dados e Fontes

### Fontes de Documentos

O sistema indexa automaticamente:

1. **Relat√≥rios do IPCC AR6**:
   - Synthesis Report
   - Working Group I (Physical Science Basis)
   - Working Group II (Impacts, Adaptation and Vulnerability)
   - Working Group III (Mitigation of Climate Change)

### Processamento de Dados

- **Chunking**: Divis√£o de documentos em segmentos de 1000 caracteres
- **Overlap**: Sobreposi√ß√£o de 200 caracteres entre chunks
- **Embeddings**: Gera√ß√£o de vetores usando Ollama embeddings
- **Indexa√ß√£o**: Armazenamento em FAISS ou Chroma

### Estrutura de Dados

```json
{
  "documents": [
    {
      "content": "Texto do documento...",
      "metadata": {
        "source": "IPCC AR6 Synthesis Report",
        "url": "https://www.ipcc.ch/report/ar6/syr/",
        "type": "website",
        "chunk_id": 1,
        "chunk_size": 1000
      }
    }
  ]
}
```

## üìà M√©tricas e Avalia√ß√£o

### M√©tricas Implementadas

#### RAGAS (RAG Assessment)
- **Faithfulness**: Fidelidade √†s fontes (meta: > 0.8)
- **Answer Relevancy**: Relev√¢ncia da resposta (meta: > 0.7)
- **Context Precision**: Precis√£o do contexto (meta: > 0.6)
- **Context Recall**: Cobertura do contexto (meta: > 0.6)

#### M√©tricas de Performance
- **Lat√™ncia**: Tempo m√©dio de resposta (meta: < 5s)
- **Throughput**: Consultas por minuto
- **Disponibilidade**: Uptime do sistema

#### M√©tricas de Qualidade
- **Cita√ß√µes**: Presen√ßa de cita√ß√µes (meta: > 80%)
- **Anti-alucina√ß√£o**: Verifica√ß√£o de evid√™ncias
- **Consist√™ncia**: Coer√™ncia entre respostas similares

### Dataset de Avalia√ß√£o

- **20+ perguntas** sobre mudan√ßas clim√°ticas
- **Respostas rotuladas** manualmente
- **M√©tricas autom√°ticas** de qualidade
- **Benchmarks** de performance

### Executar Avalia√ß√£o

```bash
# Avalia√ß√£o completa
python -m eval.evaluation

# Avalia√ß√£o espec√≠fica
python -c "
from eval.evaluation import ClimateAssistantEvaluator
from ingest.document_processor import DocumentProcessor
from src.agents import ClimateAssistantAgents

# Inicializar componentes
doc_processor = DocumentProcessor()
doc_processor.load_vector_store()
agents = ClimateAssistantAgents(doc_processor)

# Executar avalia√ß√£o
evaluator = ClimateAssistantEvaluator(doc_processor, agents)
results = evaluator.run_comprehensive_evaluation()
print(results)
"
```

## üìä Uso da API

### Endpoints Principais

#### `/get_response` (POST)
Endpoint principal para chat, compat√≠vel com frontend existente.

**Request:**
```json
{
  "messages": [
    {"role": "user", "content": "Quais s√£o as principais evid√™ncias do aquecimento global?"}
  ]
}
```

**Response:**
```json
{
  "response": "Resposta com cita√ß√µes...",
  "response_html": "<p>Resposta formatada em HTML...</p>",
  "metadata": {
    "citations_count": 3,
    "retrieved_docs_count": 5,
    "citations": [...]
  }
}
```

#### `/search` (POST)
Busca direta em documentos.

#### `/status` (GET)
Status do sistema.

#### `/health` (GET)
Health check.

#### `/reload` (POST)
Recarregar o sistema (√∫til para desenvolvimento).

### Exemplo de Uso
```python
import requests

# Fazer pergunta sobre mudan√ßas clim√°ticas
response = requests.post('http://localhost:5000/get_response', json={
    'messages': [
        {'role': 'user', 'content': 'Como o IPCC define mudan√ßas clim√°ticas?'}
    ]
})

print(response.json()['response'])
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente
```bash
# LLM Configuration
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=llama3.1:8b
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector Store
VECTOR_STORE_TYPE=faiss  # ou chroma
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API
API_HOST=localhost
API_PORT=5000
DEBUG=False

# Agent Configuration
MAX_ITERATIONS=10
TEMPERATURE=0.1
```

## üê≥ Docker

### Construir e Executar
```bash
# Construir imagem
docker build -t climate-assistant .

# Executar container
docker run -p 5000:5000 climate-assistant

# Com docker-compose
docker-compose up --build
```

### Docker Compose

```yaml
version: '3.8'
services:
  climate-assistant:
    build: .
    ports:
      - "5000:5000"
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
      - LLM_MODEL=llama3.1:8b
    volumes:
      - ./data:/app/data
```

## üß™ Desenvolvimento

### Estrutura de Desenvolvimento

```bash
# Instalar depend√™ncias de desenvolvimento
pip install -r requirements.txt

# Executar testes
python -m pytest tests/ -v

# Executar linting
python -m flake8 src/ app/ ingest/ eval/

# Executar type checking
python -m mypy src/ app/ ingest/ eval/
```

# Adicionar ao grafo
workflow.add_node("new_agent", self.new_agent)
workflow.add_edge("previous_agent", "new_agent")
```

### Adicionando Novas Fontes

```python
# src/config.py
DOCUMENT_SOURCES.append({
    "name": "Nova Fonte",
    "url": "https://example.com",
    "type": "website"
})
```

## ü§ù Contribui√ß√£o

### Desenvolvimento
1. Fork o reposit√≥rio
2. Crie uma branch para sua feature
3. Implemente testes
4. Execute avalia√ß√£o completa
5. Submeta pull request

### Melhorias Futuras
- [ ] Suporte a mais idiomas
- [ ] Integra√ß√£o com mais fontes de dados
- [ ] Interface web melhorada
- [ ] M√©tricas em tempo real
- [ ] Cache inteligente
- [ ] Suporte a documentos PDF
- [ ] API GraphQL
- [ ] Autentica√ß√£o e autoriza√ß√£o

## üìÑ Licen√ßa

Este projeto √© open-source e est√° dispon√≠vel sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## üÜò Suporte

### Problemas Comuns

#### Ollama n√£o responde
```bash
# Verificar se est√° rodando
ollama list

# Reiniciar servi√ßo
ollama serve
```

#### Erro de mem√≥ria
- Reduza o tamanho do modelo LLM
- Use chunk_size menor
- Aumente RAM dispon√≠vel

#### Documentos n√£o carregam
- Verifique conex√£o com internet
- Confirme URLs das fontes
- Execute `python -m ingest.document_processor` novamente

#### Erros de importa√ß√£o
- Verifique se est√° executando do diret√≥rio correto
- Use `python -m` para executar m√≥dulos
- Verifique se todos os `__init__.py` est√£o presentes

### Logs
```bash
# Ver logs detalhados
tail -f logs/climate_assistant.log

# Logs do Docker
docker logs climate-assistant
```

## üìö Refer√™ncias

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Ollama Documentation](https://ollama.ai/docs)
- [IPCC Reports](https://www.ipcc.ch/reports/)
- [RAGAS Evaluation](https://github.com/explodinggradients/ragas)
- [FAISS Documentation](https://faiss.ai/)
- [Chroma Documentation](https://docs.trychroma.com/)

---
