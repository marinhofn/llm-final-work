# Climate Assistant RAG System

Um assistente inteligente com RAG (Retrieval-Augmented Generation) + Agentes para questÃµes de meio ambiente e mudanÃ§as climÃ¡ticas, baseado em documentos do IPCC e outras fontes cientÃ­ficas.

## ğŸŒ VisÃ£o Geral

Este sistema implementa uma prova de conceito (PoC) de um assistente com:
- **RAG**: RecuperaÃ§Ã£o de informaÃ§Ãµes de documentos cientÃ­ficos
- **Agentes LangGraph**: OrquestraÃ§Ã£o inteligente de mÃºltiplos agentes especializados
- **Anti-alucinaÃ§Ã£o**: Sistema de verificaÃ§Ã£o e citaÃ§Ãµes obrigatÃ³rias
- **Foco Ambiental**: Especializado em mudanÃ§as climÃ¡ticas e meio ambiente

## ğŸ—ï¸ Arquitetura

### Agentes LangGraph
1. **Supervisor**: Roteamento e gerenciamento de consultas
2. **Retriever**: Busca e recuperaÃ§Ã£o de documentos relevantes
3. **Answerer**: GeraÃ§Ã£o de respostas com citaÃ§Ãµes obrigatÃ³rias
4. **Self-check**: VerificaÃ§Ã£o de qualidade e evidÃªncias
5. **Safety**: AdiÃ§Ã£o de disclaimers e verificaÃ§Ãµes de seguranÃ§a

### Stack TecnolÃ³gica
- **Python 3.8+**
- **LangChain + LangGraph**: OrquestraÃ§Ã£o de agentes
- **Ollama**: LLM open-weights (Llama 3.1 8B, Qwen2.5 7B, etc.)
- **FAISS/Chroma**: Armazenamento vetorial
- **HuggingFace Embeddings**: Embeddings de texto
- **Flask**: API REST compatÃ­vel com frontend existente

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos
- Python 3.8 ou superior
- Ollama instalado e configurado
- 8GB+ RAM recomendado

### 1. InstalaÃ§Ã£o AutomÃ¡tica
```bash
cd backend
python setup.py
```

### 2. InstalaÃ§Ã£o Manual

#### Instalar dependÃªncias Python:
```bash
pip install -r requirements.txt
```

#### Instalar e configurar Ollama:
```bash
# Instalar Ollama (https://ollama.ai/)
# Baixar modelo LLM
ollama pull llama3.1:8b
# ou
ollama pull qwen2.5:7b
```

#### Processar documentos:
```bash
python document_processor.py
```

#### Iniciar servidor:
```bash
python app.py
```

## ğŸ“Š Uso

### API Endpoints

#### `/get_response` (POST)
Endpoint principal para chat, compatÃ­vel com frontend existente.

**Request:**
```json
{
  "messages": [
    {"role": "user", "content": "Quais sÃ£o as principais evidÃªncias do aquecimento global?"}
  ]
}
```

**Response:**
```json
{
  "response": "Resposta com citaÃ§Ãµes...",
  "metadata": {
    "citations_count": 3,
    "retrieved_docs_count": 5,
    "citations": [...]
  }
}
```

#### `/search` (POST)
Busca direta em documentos.

#### `/status` (GET)
Status do sistema.

#### `/health` (GET)
Health check.

### Exemplo de Uso
```python
import requests

# Fazer pergunta sobre mudanÃ§as climÃ¡ticas
response = requests.post('http://localhost:5000/get_response', json={
    'messages': [
        {'role': 'user', 'content': 'Como o IPCC define mudanÃ§as climÃ¡ticas?'}
    ]
})

print(response.json()['response'])
```

## ğŸ“ˆ AvaliaÃ§Ã£o

### Executar AvaliaÃ§Ã£o Completa
```bash
python evaluation.py
```

### MÃ©tricas Implementadas
- **RAGAS**: Faithfulness, Answer Relevancy, Context Precision/Recall
- **LatÃªncia**: Tempo mÃ©dio de resposta
- **CitaÃ§Ãµes**: Qualidade e presenÃ§a de citaÃ§Ãµes
- **Anti-alucinaÃ§Ã£o**: VerificaÃ§Ã£o de evidÃªncias

### Dataset de AvaliaÃ§Ã£o
- 20+ perguntas sobre mudanÃ§as climÃ¡ticas
- Respostas rotuladas manualmente
- MÃ©tricas automÃ¡ticas de qualidade

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente
```bash
# LLM Configuration
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=llama3.1:8b
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector Store
VECTOR_STORE_TYPE=faiss  # ou chroma
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API
API_HOST=localhost
API_PORT=5000
DEBUG=False
```

### Fontes de Documentos
O sistema indexa automaticamente:
- RelatÃ³rios do IPCC AR6
- Documentos cientÃ­ficos sobre clima
- Bases de dados ambientais abertas

## ğŸ›ï¸ Estrutura do Projeto

```
backend/
â”œâ”€â”€ app.py                 # API Flask principal
â”œâ”€â”€ agents.py             # Agentes LangGraph
â”œâ”€â”€ config.py             # ConfiguraÃ§Ãµes
â”œâ”€â”€ document_processor.py # Processamento de documentos
â”œâ”€â”€ evaluation.py         # Sistema de avaliaÃ§Ã£o
â”œâ”€â”€ setup.py             # Script de instalaÃ§Ã£o
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ README.md           # Esta documentaÃ§Ã£o
â”œâ”€â”€ data/               # Dados e documentos
â”‚   â”œâ”€â”€ documents/      # Documentos originais
â”‚   â”œâ”€â”€ vector_store/   # Ãndices vetoriais
â”‚   â””â”€â”€ evaluation/     # Resultados de avaliaÃ§Ã£o
â””â”€â”€ Dockerfile         # ContainerizaÃ§Ã£o
```

## ğŸ”’ Ã‰tica e SeguranÃ§a

### Disclaimers AutomÃ¡ticos
- InformaÃ§Ãµes apenas para fins educacionais
- RecomendaÃ§Ã£o de consultar especialistas
- Aviso sobre limitaÃ§Ãµes dos dados

### LimitaÃ§Ãµes
- NÃ£o fornece diagnÃ³stico mÃ©dico
- NÃ£o oferece assessoria legal
- Sempre informativo, nunca prescritivo

## ğŸ³ Docker

### Construir e Executar
```bash
# Construir imagem
docker build -t climate-assistant .

# Executar container
docker run -p 5000:5000 climate-assistant
```

## ğŸ“Š MÃ©tricas de Performance

### Benchmarks Esperados
- **LatÃªncia**: < 5s por consulta
- **Faithfulness**: > 0.8
- **Answer Relevancy**: > 0.7
- **Context Precision**: > 0.6
- **CitaÃ§Ãµes**: > 80% das respostas

### Monitoramento
- Logs detalhados de cada agente
- MÃ©tricas de latÃªncia por componente
- Qualidade das citaÃ§Ãµes

## ğŸ¤ ContribuiÃ§Ã£o

### Desenvolvimento
1. Fork o repositÃ³rio
2. Crie uma branch para sua feature
3. Implemente testes
4. Execute avaliaÃ§Ã£o completa
5. Submeta pull request

### Melhorias Futuras
- [ ] Suporte a mais idiomas
- [ ] IntegraÃ§Ã£o com mais fontes de dados
- [ ] Interface web melhorada
- [ ] MÃ©tricas em tempo real
- [ ] Cache inteligente

## ğŸ“„ LicenÃ§a

Este projeto Ã© open-source e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.

## ğŸ†˜ Suporte

### Problemas Comuns

#### Ollama nÃ£o responde
```bash
# Verificar se estÃ¡ rodando
ollama list

# Reiniciar serviÃ§o
ollama serve
```

#### Erro de memÃ³ria
- Reduza o tamanho do modelo LLM
- Use chunk_size menor
- Aumente RAM disponÃ­vel

#### Documentos nÃ£o carregam
- Verifique conexÃ£o com internet
- Confirme URLs das fontes
- Execute document_processor.py novamente

### Logs
```bash
# Ver logs detalhados
tail -f logs/climate_assistant.log
```

## ğŸ“š ReferÃªncias

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Ollama Documentation](https://ollama.ai/docs)
- [IPCC Reports](https://www.ipcc.ch/reports/)
- [RAGAS Evaluation](https://github.com/explodinggradients/ragas)

---

**Desenvolvido com â¤ï¸ para promover conhecimento sobre mudanÃ§as climÃ¡ticas**
